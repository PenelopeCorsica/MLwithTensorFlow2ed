{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Deep Convolutional Neural Network and Optimizations\n",
    "I originally resisted writing documentation on this, but since this is really the predecessor to the end of chapter assignent for building a face detector, and since I put in a ton of work into understanding CNN's and even trying out and testing different models, it was really worth developing some good documentation for this and I built a variety of links to really useful examples and documentation. \n",
    "\n",
    "The [CIFAR (Canadian Institute For Advanced Research)](https://www.cs.toronto.edu/~kriz/cifar.html) CIFAR-10 dataset is a big focus of the chapter. It was collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Alex Krizhevsky was the author of the seminal paper in using Convolutional Neural Networks (CNN) for Deep Learning and Image Classification. The [paper from NIPS 2012 - ImageNet Classification with Deep Convolutional Neural Networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) is a seminal paper and has been cited 40,000+ times. In the paper, Krizhevsky talks about many of the foundational building blocks that Nishant covers in Chapter 9 in the book. For example, he introduces:\n",
    "\n",
    "   1. Deep Layered Architectures and Convolutions for Image Recognition.\n",
    "   2. The use of Rectifying Linear Unit (ReLU) as a mechanism for smoothing out activation of neurons as particular well suited to image processing.\n",
    "   3. Image Augmentation activities including flipping the image left and right, rotating it, adding salt and pepper, etc.\n",
    "   4. Adding MaxPool layers after the Convolutional Layers. \n",
    "   5. Batch Normalization technique to smooth out the learning process.\n",
    "   \n",
    "The first part of chapter 15 is focused on teaching us basically what is a variation of `AlexNet`, which is Krizhevsky's famous CNN architecture for image processing. Besides using it for CIFAR-10, `AlexNet` was used to win the ImageNet 2012 challenge. [ImageNet](http://www.image-net.org/) is a corpora of millions of images labeled with the [WordNet](https://wordnet.princeton.edu/) taxonomy with 1000 classes. \n",
    "\n",
    "I focused a lot early on in the chapter just figuring out what convolutions actually do. They scan an image L-R, then T-B, using a window of a particular `size` which turns out to be the size of the feature vector. In this case, it is `[W x H x C x Cv]` where `W` is conv width, `H` is conv height, `C` is num channels and `Cv` is the number of convolutional filters. The original RGB CIFAR images have `C=3`, however, one of the early image cleansing functions taught in the book is to turn the images to greyscale, or `C=1`. Funny enough many of the examples I found online for CIFAR just went ahead and used the RGB channels - all 3 - as they were. I believe this is likely due to the fact that you can learn something about CIFAR's 10 classes, `[airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck]` based on the color channels too. The `Cv` parameter is an interesting one. It is basically a hyper parameter, since the book uses different ones, e.g., `32`, I've seen `64` (and ended up using it based on online examples). Also the density of the network's \"deepness\" is also really a learned parameter. The convolutions and max pool layers after that is really a representation of the lower level at first (e.g., curves, lines, triangles/angles) and then eventually higher level features (paws, wheels, etc.). \n",
    "\n",
    "Probably my big ah-ha moment was reading this amazing [Beginner's Guide to Understanding Convolutional Neural Networks](https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/) by [Adit Deshpande (UCLA CS Student)](https://adeshpande3.github.io/adeshpande3.github.io/). Fantastic way to understand CNN's and I fully recommend it.\n",
    "\n",
    "After building a basic convolution network with [Listing 14.14-16](../ch14/Listing%214.11%20-%214.16.ipynb) in the book, I was able to achieve a `77% ROC micro-average accuracy` across the 10 classes, and on some (`automobiles`) as high as `85%`. What was amazing though was to even do a 1000-epoch training with batch size set to `250`, it took around 30+ hours to train on my local CPU (MacBook Pro 2.9 GHz Intel Core i9 32 GB 2400 MHz DDR4 RAM). I had to basically get some GPU time on a supercomputer to train it in a few hours. My test / evaluation accuracy performance was lower though in the `59%` range. In the book it then suggests adding any of the following to the example:\n",
    "\n",
    "   1. Augmenting data - I did this, see above and the `augment` function below. I lifted the `sp_noise` function from the [linked example](https://stackoverflow.com/questions/22937589/how-to-add-noise-gaussian-salt-and-pepper-etc-to-image-in-python-with-opencv).\n",
    "   2. Early stopping - I could see why this is important, I could have implemented a simple check that showed when the loss only changes by a very small fraction across iterations and stopped, but I didn't.\n",
    "   3. Regularizing weights—I found a [great tutorial on Regularization using TF native functions](https://www.ritchieng.com/machine-learning/deep-learning/tensorflow/regularization/) and applied it - easier than the earlier example in the book.\n",
    "   4. Dropout—Figuring out what layer to use this in is the most important. Using it in the FC (fully connected layers) worked best for this CNN architecture I was testing.\n",
    "   5. Deeper architecture — I found quite a few examples of other Deep architectures and CNNs using Tensorflow, and Keras, and other examples for CIFAR-10. I'll detail some of the take-aways below, but the new `model()` function is literally lifted from [this fantastic tutorial](https://towardsdatascience.com/cifar-10-image-classification-in-tensorflow-5b501f7dc77c).\n",
    "   \n",
    "After making these series of updates, I was able to get to a `80% ROC micro-average accuracy` and to a test accuracy of `64%`, which were some nice improvements. I also added some code to \"test it in the wild\" on random image URLs and it performed decent. I'll explain the motivation and some of the cool links and architectures I found that influenced the final solution.\n",
    "\n",
    "## Towards Data Science CIFAR-10 CNN\n",
    "[This particular CNN notebook](https://towardsdatascience.com/cifar-10-image-classification-in-tensorflow-5b501f7dc77c) was fantastic and easy to understand. It's a deep architecture, keeps the images as color, has some great tutorials on what the data actually looks like and what the transforms are doing. It also has a [great graphic](https://miro.medium.com/max/504/1*SmfhKWHXHVEMg8KqNaj-uw.gif) of what is actually happening during training, weight updates, and back propagation. Some aspects of this notebook are that it achieved ~`75%` prediction accuracy during training, but overall during testing was only about `59%`. I really liked the way it displayed predictions [as shown at the bottom of this notebook](https://github.com/deep-diver/CIFAR10-img-classification-tensorflow/blob/master/CIFAR10_image_classification.ipynb), so I incorporated that into the `evaluate_model` function at the end of the notebook. The dense architecture incorporates dropout, batch normalization, but doesn't use regularization. It also doesn't do data augmentation. In my own version of it, I added these. The architecture of this network is shown below.\n",
    "![ch15-cnn-arch](../figs/ch15-cnn-arch.png)\n",
    "\n",
    "## Tensorflow's own Deep CNN CIFAR-10 Tutorial\n",
    "Tensorflow itself publishes [its own Deep CNN CIFAR-10 Tutorial](https://www.tensorflow.org/tutorials/images/deep_cnn). The things I learned from this tutorial are:\n",
    "   1. How to perform data augmentation, e.g, Tensorflow's own [tf.random_crop](https://www.tensorflow.org/api_docs/python/tf/image/random_crop), and other functions including [tf.random_flip_left_right](https://www.tensorflow.org/api_docs/python/tf/image/random_flip_left_right), and other things. The great things about these functions is that they operate on tensors, but that also suggests that you need a sort of lazy preprocessing that I didn't feel like implementing and lazy iterators that they use in their notebooks. All advanced stuff, but not necessary for my own exploration. \n",
    "   2. How to elegantly use `tf.scope` (like we did during the Ch07 AutoEncoder work) to separate out the training and testing/eval portions of the work. \n",
    "   3. A [deep CNN model architecture](https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10.py#L163) very similar to the towards data science model previously shown but also does not use as many convolutions. The architecture is shown below.\n",
    "   4. How to train with multiple-GPUs (elegant, as shown in [here](https://www.tensorflow.org/tutorials/images/deep_cnn#launching_and_training_the_model_on_multiple_gpu_cards)).\n",
    "   \n",
    "Overall a very useful tutorial and suggests that at peak can achieve `86%` accuracy with a few hours of training on GPUs (I am still in the process of testing this out).\n",
    "![ch15-tensorflow-cnn](https://www.tensorflow.org/images/cifar_graph.png)\n",
    "\n",
    "## Keras Based CIFAR-10 Deep Learning Tutorial\n",
    "This [fantastic tutorial](https://appliedmachinelearning.blog/2018/03/24/achieving-90-accuracy-in-object-recognition-task-on-cifar-10-dataset-with-keras-convolutional-neural-networks/) shows how to use [Keras](http://keras.io) for creating a CIFAR-10 Deep Learning CNN. Some of the cool properties about it include:\n",
    "\n",
    "   1. They do data augmentation using Keras's own [ImageDataGenerator](https://keras.io/preprocessing/image/) which can also be used in Tensorflow. I later found a great tutorial on this [here](https://www.pyimagesearch.com/2019/07/08/keras-imagedatagenerator-and-data-augmentation/).\n",
    "   2. They use regularization.\n",
    "   3. They use dropout.\n",
    "   4. Their network architecture is similar, but also smaller compared to the Deep 14 layer architecture I ended up implementing. \n",
    "   5. They use batch normalization.\n",
    "   6. They show Keras looks a lot like Tensorflow and they can even work together.\n",
    "   \n",
    "According to their results, the model achieves `90%` accuracy though I haven't verified this.\n",
    "![ch15-keras-cnn](https://i0.wp.com/appliedmachinelearning.blog/wp-content/uploads/2018/03/cnn.png?zoom=2&resize=698%2C282&ssl=1)\n",
    "\n",
    "## Conclusion \n",
    "I spent so much time on trying to nail CIFAR-10, that the face detection end of chapter assignemnt from chapter 14 became sort of an afterthought. I figured if I could implement all the extras and do all the extra research in this assignment that it would pay off in the end. We'll see I'm still working on the end of chapter assignment. Some nice things I did in this notebook:\n",
    "\n",
    "   1. My model includes all of the suggested improvements, save early stopping (e.g., Augmentation, Regularization, Deeper Architecture, Dropout, and I even added Batch Normalization). \n",
    "   2. I added simple predict functions.\n",
    "   3. I incorporated ROC curves to understand how well the model is performing per class.\n",
    "   4. I ended up learning so much about CIFAR and the tutorial that I fixed [a bug](https://github.com/tensorflow/models/pull/7227) or [two](https://github.com/tensorflow/models/pull/7226) in [Google's Tensorflow Models CIFAR-10 tutorial](https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10).\n",
    "   \n",
    "Enjoy!\n",
    "\n",
    "## References\n",
    "\n",
    "   0. [The CIFAR-10 and CIFAR-100 Datasets](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "   1. [Data Augmentation and PyImageSearch](https://www.pyimagesearch.com/2019/07/08/keras-imagedatagenerator-and-data-augmentation/)\n",
    "   2. [AlexNet Wikipedia Article](https://en.wikipedia.org/wiki/AlexNet) and [Seminal AlexNet paper from NIPS 2012](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n",
    "   3. [Data Augmentation - Adding Gaussian Noise to an Image](https://stackoverflow.com/questions/22937589/how-to-add-noise-gaussian-salt-and-pepper-etc-to-image-in-python-with-opencv)\n",
    "   4. [Data Augmentation - Random Image Crop and Tensorflow](https://stackoverflow.com/questions/42147427/tensorflow-how-to-randomly-crop-input-images-and-labels-in-the-same-way)\n",
    "   5. [Data Augmentation - Random Image Flip](https://stackoverflow.com/questions/9154120/how-can-i-flip-an-image-along-the-vertical-axis-with-python)\n",
    "   6. [Data Augmentation - Great SkImage Tutorial on Image Processing](https://scikit-image.org/docs/dev/user_guide/transforming_image_data.html)\n",
    "   7. [Data Augmentation - SkImage More Documentation](https://scikit-image.org/docs/dev/api/skimage.util.html)\n",
    "   8. [Deep Architecture - Tensorflow CIFAR-10 Tutorial Multi GPUs](https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py)\n",
    "   9. [Representing the Answer - how to use MatplotLib for Multi-class Classification and ROC curves](https://scikit-learn.org/0.15/auto_examples/plot_roc.html)\n",
    "   10. [How to get a Tensor by Name - Tensorflow](https://stackoverflow.com/questions/36612512/tensorflow-how-to-get-a-tensor-by-name)\n",
    "   11. [Regularization Tutorial using Tensorflow and Multi-class](https://www.ritchieng.com/machine-learning/deep-learning/tensorflow/regularization/)\n",
    "   12. [Testing the Model - Turning an Image URL into a usable Numpy Array](https://www.pyimagesearch.com/2015/03/02/convert-url-to-image-with-python-and-opencv/)\n",
    "   13. [Data Augmentation - use SkImage to rescale an Image](https://stackoverflow.com/questions/48121916/numpy-resize-rescale-image)\n",
    "   14. [Keras - A fantastic CIFAR-10 tutorial on how to design CNN models and how to decide when and where to add improvement steps](https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/)\n",
    "   15. [Data Augmentation - Using Scipy for Imae processing](http://scipy-lectures.org/advanced/image_processing/)\n",
    "   16. [Diff between tf.contrib.flatten_layers and resizing using np](https://stackoverflow.com/questions/49406654/tf-reshape-vs-tf-contrib-layers-flatten)\n",
    "   17. [Deep Architecture - Fantastic Towards Data Science Tutorial](https://towardsdatascience.com/cifar-10-image-classification-in-tensorflow-5b501f7dc77c) and its associated [notebook](https://github.com/deep-diver/CIFAR10-img-classification-tensorflow/blob/master/CIFAR10_image_classification.ipynb)\n",
    "   18. [The Vanishing Gradient Problem and Why we do Regularization and other Steps](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tf_slim.layers import layers as _layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "from skimage import color\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    dict = pickle.load(fo, encoding='latin1')\n",
    "    fo.close()\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_noise(image,prob):\n",
    "    '''\n",
    "    Add salt and pepper noise to image\n",
    "    prob: Probability of the noise\n",
    "    '''\n",
    "    output = np.zeros(image.shape,np.float32)\n",
    "    thres = 1 - prob \n",
    "    for i in range(image.shape[0]):\n",
    "        for j in range(image.shape[1]):\n",
    "            rdn = random.random()\n",
    "            if rdn < prob:\n",
    "                output[i][j] = 0.\n",
    "            elif rdn > thres:\n",
    "                output[i][j] = 255.\n",
    "            else:\n",
    "                output[i][j] = image[i][j]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(img_data, img_labels):\n",
    "    imgs = img_data.reshape(img_data.shape[0], 3, 32, 32)\n",
    "    flip_pct = 0.05 # 5% chance of flipping the image each time\n",
    "    salt_pct = 0.05 # 5% chance of noise\n",
    "    noise_pct = 0.15 # salt and pepper noise\n",
    "    \n",
    "    orig_size = len(imgs)\n",
    "    for i in tqdm(range(0, orig_size)):\n",
    "        # random flip UD (which will in effect do the LR flip)\n",
    "        if random.random() < flip_pct:\n",
    "            #print('IMFLIP: im['+str(i)+']')\n",
    "            im_flip = np.expand_dims(np.flipud(imgs[i]), axis=0)\n",
    "            imgs = np.vstack((imgs, im_flip))\n",
    "            img_labels = np.hstack((img_labels, img_labels[i]))\n",
    "        \n",
    "        # random salt and pepper noise\n",
    "        if random.random() < salt_pct:\n",
    "            #print('IMSALT: im['+str(i)+']')\n",
    "            im_salt = np.expand_dims(sp_noise(imgs[i], noise_pct), axis=0)\n",
    "            imgs = np.vstack((imgs, im_salt))\n",
    "            img_labels = np.hstack((img_labels, img_labels[i]))\n",
    "            \n",
    "    return imgs.reshape(imgs.shape[0], -1), img_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(data):\n",
    "    imgs = data.reshape(data.shape[0], 3, 32, 32)\n",
    "    grayscale_imgs = imgs.mean(1)\n",
    "    cropped_imgs = grayscale_imgs[:, 4:28, 4:28]\n",
    "    img_data = cropped_imgs.reshape(imgs.shape[0], -1)\n",
    "    img_size = np.shape(img_data)[1]\n",
    "    means = np.mean(img_data, axis=1)\n",
    "    meansT = means.reshape(len(means), 1)\n",
    "    stds = np.std(img_data, axis=1)\n",
    "    stdsT = stds.reshape(len(stds), 1)\n",
    "    adj_stds = np.maximum(stdsT, 1.0 / np.sqrt(img_size))\n",
    "    normalized = (img_data - meansT) / adj_stds\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(directory):\n",
    "    data_file = '../../data/cache/aug_data.npy'\n",
    "    labels_file = '../../data/cache/aug_labels.npy'\n",
    "\n",
    "    names = unpickle('{}/batches.meta'.format(directory))['label_names']\n",
    "    print('names', names)\n",
    "    data, labels = [], []\n",
    "    \n",
    "    if os.path.exists(data_file) and os.path.isfile(data_file) and os.path.exists(labels_file) and os.path.isfile(labels_file):\n",
    "            print('Loading data from cache files {} and {}'.format(data_file, labels_file))\n",
    "            data = np.load(data_file)\n",
    "            labels = np.load(labels_file)\n",
    "    else:\n",
    "        for i in range(1, 6):\n",
    "            filename = '{}/data_batch_{}'.format(directory, i)\n",
    "            batch_data = unpickle(filename)\n",
    "            if len(data) > 0:\n",
    "                data = np.vstack((data, batch_data['data']))\n",
    "                labels = np.hstack((labels, batch_data['labels']))\n",
    "            else:\n",
    "                data = batch_data['data']\n",
    "                labels = batch_data['labels']\n",
    "\n",
    "        data, labels = augment(data, labels)\n",
    "        data = clean(data)        \n",
    "        data = data.astype(np.float32)\n",
    "        np.save('../../data/cache/aug_data.npy', data)\n",
    "        np.save('../../data/cache/aug_labels.npy', labels)\n",
    "\n",
    "    print(np.shape(data), np.shape(labels))\n",
    "\n",
    "    return names, data, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
      "Loading data from cache files ../../data/cache/aug_data.npy and ../../data/cache/aug_labels.npy\n",
      "(55153, 576) (55153,)\n"
     ]
    }
   ],
   "source": [
    "names, data, labels = read_data('../../data/cifar-10-batches-py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Image of \"+str(names[labels[52001]]))\n",
    "img = np.reshape(data[52001, :], (24,24))\n",
    "plt.imshow(img, cmap='Greys_r')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "rate=0.3\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(model, inputs, outputs):\n",
    "    beta = 0.1 #regularlization\n",
    "    weights = [model.conv1_filter, model.conv2_filter, model.conv3_filter, model.conv4_filter]\n",
    "    regularizer = tf.nn.l2_loss(weights[0])\n",
    "    for w in range(1, len(weights)):\n",
    "        regularizer = regularizer + tf.nn.l2_loss(weights[w])\n",
    "\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model(inputs), labels=outputs))\n",
    "    cost = tf.reduce_mean(cost + beta * regularizer) #L2 regularization\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, inputs, outputs):\n",
    "        with tf.GradientTape() as t:\n",
    "            current_loss = loss(model, inputs, outputs)\n",
    "\n",
    "        grads = t.gradient(current_loss, [model.conv1_filter, model.conv2_filter, model.conv3_filter, model.conv4_filter])\n",
    "        optimizer.apply_gradients(zip(grads,[model.conv1_filter, model.conv2_filter, model.conv3_filter, model.conv4_filter]))\n",
    "        return current_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_predictions(model, inputs, outputs):\n",
    "    return tf.equal(tf.argmax(model(inputs), 1), tf.argmax(outputs, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(correct_pred):\n",
    "    return tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCNNModel(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.conv1_filter = tf.Variable(tf.random.truncated_normal(shape=[3, 3, 1, 64], mean=0, stddev=0.08)) #changed 3 to 1 in 3rd param b/c Grey\n",
    "        self.conv2_filter = tf.Variable(tf.random.truncated_normal(shape=[3, 3, 64, 128], mean=0, stddev=0.08))\n",
    "        self.conv3_filter = tf.Variable(tf.random.truncated_normal(shape=[5, 5, 128, 256], mean=0, stddev=0.08))\n",
    "        self.conv4_filter = tf.Variable(tf.random.truncated_normal(shape=[5, 5, 256, 512], mean=0, stddev=0.08))\n",
    "        \n",
    "\n",
    "    def __call__(self, x, rate=rate):\n",
    "        \n",
    "        x_reshaped = tf.reshape(x, shape=[-1, 24, 24, 1])\n",
    "\n",
    "        conv1 = tf.nn.conv2d(x_reshaped, self.conv1_filter, strides=[1,1,1,1], padding='SAME')\n",
    "        conv1 = tf.nn.relu(conv1)\n",
    "        conv1_pool = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "        conv1_bn = tf.keras.layers.BatchNormalization()(conv1_pool)\n",
    "\n",
    "        conv2 = tf.nn.conv2d(conv1_bn, self.conv2_filter, strides=[1,1,1,1], padding='SAME')\n",
    "        conv2 = tf.nn.relu(conv2)\n",
    "        conv2_pool = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')    \n",
    "        conv2_bn = tf.keras.layers.BatchNormalization()(conv2_pool)\n",
    "\n",
    "        conv3 = tf.nn.conv2d(conv2_bn, self.conv3_filter, strides=[1,1,1,1], padding='SAME')\n",
    "        conv3 = tf.nn.relu(conv3)\n",
    "        conv3_pool = tf.nn.max_pool(conv3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')  \n",
    "        conv3_bn = tf.keras.layers.BatchNormalization()(conv3_pool)\n",
    "\n",
    "        conv4 = tf.nn.conv2d(conv3_bn, self.conv4_filter, strides=[1,1,1,1], padding='SAME')\n",
    "        conv4 = tf.nn.relu(conv4)\n",
    "        conv4_pool = tf.nn.max_pool(conv4, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "        conv4_bn = tf.keras.layers.BatchNormalization()(conv4_pool)\n",
    "\n",
    "        flat = _layers.flatten(conv4_bn)  \n",
    "\n",
    "        full1 = _layers.fully_connected(inputs=flat, num_outputs=128, activation_fn=tf.nn.relu)\n",
    "        full1 = tf.nn.dropout(full1, rate=rate)\n",
    "        full1 = tf.keras.layers.BatchNormalization()(full1)\n",
    "\n",
    "        full2 = _layers.fully_connected(inputs=full1, num_outputs=256, activation_fn=tf.nn.relu)\n",
    "        full2 = tf.nn.dropout(full2, rate=rate)\n",
    "        full2 = tf.keras.layers.BatchNormalization()(full2)\n",
    "\n",
    "        full3 = _layers.fully_connected(inputs=full2, num_outputs=512, activation_fn=tf.nn.relu)\n",
    "        full3 = tf.nn.dropout(full3, rate=rate)\n",
    "        full3 = tf.keras.layers.BatchNormalization()(full3)    \n",
    "\n",
    "        full4 = _layers.fully_connected(inputs=full3, num_outputs=1024, activation_fn=tf.nn.relu)\n",
    "        full4 = tf.nn.dropout(full4, rate=rate)\n",
    "        full4 = tf.keras.layers.BatchNormalization()(full4)        \n",
    "\n",
    "        out = _layers.fully_connected(inputs=full4, num_outputs=10, activation_fn=None)\n",
    "        \n",
    "        out = tf.identity(out, name='logits')\n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepCNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer, \n",
    "                          conv1_filter=model.conv1_filter,\n",
    "                          conv2_filter=model.conv2_filter,\n",
    "                          conv3_filter=model.conv3_filter,\n",
    "                          conv4_filter=model.conv4_filter)\n",
    "manager = tf.train.CheckpointManager(ckpt, '../../models/cifar10-cnn-tf1n-ia-dropout-reg-dense-'+str(epochs)+'epochs.ckpt', max_to_keep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size 275\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741bf7372bb84982bbf644cee2bb6f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-e39fe2fb1742>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mbatch_onehot_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monehot_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_onehot_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcorrect_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_onehot_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-19eac86a1b49>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(model, inputs, outputs)\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1_filter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2_filter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3_filter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv4_filter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1_filter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2_filter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3_filter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv4_filter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcurrent_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/py3-tf2/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1065\u001b[0m                           for x in nest.flatten(output_gradients)]\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1068\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/py3-tf2/lib/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     72\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/py3-tf2/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    160\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/py3-tf2/lib/python3.8/site-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    585\u001b[0m   \u001b[0;31m# in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m   return [\n\u001b[0;32m--> 587\u001b[0;31m       gen_nn_ops.conv2d_backprop_input(\n\u001b[0m\u001b[1;32m    588\u001b[0m           \u001b[0mshape_0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m           \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/py3-tf2/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_input\u001b[0;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1245\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   1248\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Conv2DBackpropInput\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_backprop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strides\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "onehot_labels = tf.one_hot(labels, len(names), on_value=1., off_value=0., axis=-1)\n",
    "batch_size = len(data) // 200\n",
    "print('batch size', batch_size)\n",
    "for j in tqdm(range(0, epochs)):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch_data = data[i:i+batch_size, :]\n",
    "        batch_onehot_vals = onehot_labels[i:i+batch_size, :]\n",
    "        err = train_step(model, batch_data, batch_onehot_vals)\n",
    "\n",
    "    correct_preds = correct_predictions(model, batch_data, batch_onehot_vals)\n",
    "    accuracy_val = accuracy(correct_preds).numpy()\n",
    "    print(\"epoch {} loss {:1.2f} accuracy {:1.2f}\".format(j, err.numpy(), accuracy_val))\n",
    "\n",
    "    if int(ckpt.step) % 100 == 0:\n",
    "        save_path = manager.save()\n",
    "        print(\"Saved checkpoint for step {}: {}\".format(int(ckpt.step), save_path))\n",
    "        print(\"loss {:1.2f}\".format(err.numpy()))\n",
    "\n",
    "save_path = manager.save()\n",
    "print(\"Saved checkpoint for step {}: {}\".format(int(ckpt.step), save_path))\n",
    "print(\"loss {:1.2f}\".format(err.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img_data):\n",
    "    class_num, class_name, confidence = None, None, 0.\n",
    "    ckpt.restore(save_path).assert_consumed()\n",
    "    print(\"Model restored.\")\n",
    "    \n",
    "    logits_out = model(img_data.reshape((1, 24*24)))\n",
    "    class_num = np.argmax(logits_out, axis=1)[0]\n",
    "    class_name = names[class_num]\n",
    "    confidence = logits_out[0,class_num]\n",
    "    all_preds = logits_out            \n",
    "            \n",
    "    return (class_num, class_name, confidence, all_preds)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num, class_name, confidence, all_preds = predict(data[3])\n",
    "print('Class Num', class_num)\n",
    "print('Class', class_name)\n",
    "print('Confidence', confidence)\n",
    "print('All Predictions', str(all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(class_name)\n",
    "img = np.reshape(data[3, :], (24,24))\n",
    "plt.imshow(img, cmap='Greys_r')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test out on test images\n",
    "def read_test_data(directory):\n",
    "    names = unpickle('{}/batches.meta'.format(directory))['label_names']\n",
    "    print('names', names)\n",
    "    data, labels = [], []\n",
    "    \n",
    "    filename = '{}/test_batch'.format(directory)\n",
    "    batch_data = unpickle(filename)\n",
    "    if len(data) > 0:\n",
    "        data = np.vstack((data, batch_data['data']))\n",
    "        labels = np.hstack((labels, batch_data['labels']))\n",
    "    else:\n",
    "        data = batch_data['data']\n",
    "        labels = batch_data['labels']\n",
    "    print(np.shape(data), np.shape(labels))\n",
    "    data = clean(data)\n",
    "    data = data.astype(np.float32)\n",
    "    return names, data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_names, test_data, test_labels = read_test_data('../../data/cifar-10-batches-py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_class_num, test_class_name, test_class_confidence, all_preds = predict(test_data[4])\n",
    "print('Test Class Num', test_class_num)\n",
    "print('Test Class Name', test_class_name)\n",
    "print('Test Class Confidence', test_class_confidence)\n",
    "print('All Preds', str(all_preds))\n",
    "print('Actual Class Label', test_labels[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(test_class_name)\n",
    "img = np.reshape(test_data[3, :], (24,24))\n",
    "plt.imshow(img, cmap='Greys_r')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_accuracy(test_data, test_names, test_labels):\n",
    "    class_num, class_name, confidence = None, None, 0.\n",
    "    ckpt.restore(save_path).assert_consumed()\n",
    "    print(\"Model restored.\")\n",
    "    \n",
    "    onehot_test_labels = tf.one_hot(test_labels, len(test_names), on_value=1., off_value=0., axis=-1).eval()              \n",
    "    test_logits_out = model(test_data)       \n",
    "    test_correct_pred = correct_pred(model, test_data, onehot_test_labels)\n",
    "    test_accuracy = accuracy(test_correct_pred)\n",
    "\n",
    "    print('Test accuracy %f' % (test_accuracy.eval()))  \n",
    "    predictions = tf.argmax(test_logits_out, 1).eval()\n",
    "    return (predictions, tf.cast(test_correct_pred, tf.float32).eval(), onehot_test_labels)\n",
    "\n",
    "    \n",
    "predict_vals, test_correct_preds, onehot_test_lbls = get_test_accuracy(test_data, test_names, test_labels)\n",
    "print(predict_vals)\n",
    "print(predict_vals.shape)\n",
    "print(test_correct_preds)\n",
    "print(test_correct_preds.shape)\n",
    "print(onehot_test_lbls.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_test = label_binarize(test_labels, classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "predictions_test = label_binarize(predict_vals, classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "n_classes = outcome_test.shape[1]\n",
    "print(n_classes)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(outcome_test[:, i], predictions_test[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "   \n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(outcome_test.ravel(), predictions_test.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Plot of a ROC curve for a specific class\n",
    "plt.figure()\n",
    "plt.plot(fpr[2], tpr[2], label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic for class: '+test_names[2])\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]))\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                                   ''.format(test_names[i], roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "roc_mean = np.mean(np.fromiter(roc_auc.values(), dtype=float))\n",
    "plt.title('ROC curve for CIFAR-10 CNN '+str(epochs)+' iter Tensorflow (area = %{0:0.2f})'.format(roc_mean))\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_img_url(url):\n",
    "    image = color.rgb2gray(imread(url))\n",
    "    new_size = 24,24\n",
    "    image = cv2.resize(image, new_size, interpolation=cv2.INTER_CUBIC)\n",
    "#    print(\"Cropped to 24x24\")\n",
    "    images = np.expand_dims(image, axis=0)\n",
    "    im_data = images.astype(np.float32)\n",
    "    prediction = predict(im_data[0])\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_urls = [\n",
    "    'http://www.torontozoo.com/adoptapond/guide_images/Green%20Frog.jpg', #frog\n",
    "    'https://cdn.cnn.com/cnnnext/dam/assets/160205192735-01-best-cruise-ships-disney-dream-super-169.jpg', #ship\n",
    "    'https://www.sailboston.com/wp-content/uploads/2016/11/amerigo-vespucci.jpg', #ship\n",
    "    'https://upload.wikimedia.org/wikipedia/commons/d/d9/Motorboat_at_Kankaria_lake.JPG', #ship\n",
    "    'https://media.wired.com/photos/5b9c3d5e7d9d332cf364ad66/master/pass/AV-Trucks-187479297.jpg', #truck\n",
    "    'https://images.schoolspecialty.com/images/1581176_ecommfullsize.jpg', #truck\n",
    "    'https://img.purch.com/w/660/aHR0cDovL3d3dy5saXZlc2NpZW5jZS5jb20vaW1hZ2VzL2kvMDAwLzEwNC84MTkvb3JpZ2luYWwvY3V0ZS1raXR0ZW4uanBn', # cat\n",
    "    'https://thehorse.com/wp-content/uploads/2017/01/iStock-510488648.jpg', #horse\n",
    "    'http://media.wired.com/photos/5d09594a62bcb0c9752779d9/master/w_2560%2Cc_limit/Transpo_G70_TA-518126.jpg' #car\n",
    "]\n",
    "\n",
    "preds=[]\n",
    "for url in predict_urls:\n",
    "    pred = predict_img_url(url)\n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(urls, predicted):\n",
    "    im_data = []\n",
    "    for url in urls:\n",
    "        image = color.rgb2gray(imread(url))\n",
    "        new_size = 24,24\n",
    "        image = cv2.resize(image, new_size, interpolation=cv2.INTER_CUBIC)\n",
    "        images = np.expand_dims(image, axis=0)\n",
    "        if len(im_data) > 0:\n",
    "            im_data = np.vstack((im_data, images.astype(np.float32)))\n",
    "        else:\n",
    "            im_data = images.astype(np.float32)\n",
    "            \n",
    "    n_predictions = len(predicted)\n",
    "    fig, axies = plt.subplots(nrows=n_predictions, ncols=2, figsize=(24, 24))\n",
    "    fig.tight_layout()\n",
    "    fig.suptitle('Softmax Predictions for '+str(len(predicted))+' CIFAR-10 CNN '+str(epochs)+' iter Image URLs', fontsize=20, y=1.1)\n",
    "\n",
    "    n_predictions = 10\n",
    "    margin = 0.05\n",
    "    ind = np.arange(n_predictions)\n",
    "    width = (1. - 2. * margin) / n_predictions\n",
    "   \n",
    "    for i in range(0, len(im_data)):\n",
    "        pred_names = names\n",
    "        pred_values = predicted[i][3][0]\n",
    "        correct_name = predicted[i][1]\n",
    "        \n",
    "        axies[i][0].imshow(im_data[i], cmap='Greys_r')\n",
    "        axies[i][0].set_title(correct_name)\n",
    "        axies[i][0].set_axis_off()\n",
    "\n",
    "        axies[i][1].barh(ind + margin, pred_values, width)\n",
    "        axies[i][1].set_yticks(ind + margin)\n",
    "        axies[i][1].set_yticklabels(pred_names)\n",
    "        axies[i][1].set_xticks([0, 0.5, 1.0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(predict_urls, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
