{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a DeepSpeech LSTM Model using the LibriSpeech Data \n",
    "At the end of Chapter 16 and into Chapter 17 in the book it is suggested try and build an automatic speech recognition system using the LibriVox corpus and long short term memory (LSTM) models just learned in the Recurrent Neural Network (RNN) chapter. This particular excercise turned out to be quite difficult mostly from the perspective again of simply gathering and formatting the data, combined with the work to understand the LSTM was doing. As it turns out, in doing this assignment I taught myself about MFCCs (mel frequency cepstral coefficient) which are simply what is going on in the  Bregman Toolkit example earlier in the book. It's a process to convert audio into *num_cepstrals* coefficients using an FFT, and to use those coeffiicents as amplitudes and convert from the frequency into the time domain. LSTMs need time series data and a number of audio files converted using MFCCs into frequency amplitudes corresponding to utterances that you have transcript data for and you are in business!\n",
    "\n",
    "The other major lesson was finding [RNN-Tutorial](https://github.com/mrubash1/RNN-Tutoria) an existing GitHub repository that implements a simplified version of the [deepspeech model](https://github.com/mozilla/DeepSpeech) from Mozilla which is a TensorFlow implementation of the Baidu model from the [seminal paper](https://arxiv.org/abs/1412.5567) in 2014.\n",
    "\n",
    "I had to figure out along the way how to tweak hyperparameters including epochs, batch size, and training data. But overall this is a great architecture and example of how to use validation/dev sets during training for looking at validation loss compared to train loss and then overall to measure test accuracy.\n",
    "\n",
    "### Data Preprocessing Steps:\n",
    "   1. Grab all text files which start out as the full speech from all subsequent \\*.flac files\n",
    "   2. Each line in the text file contains:\n",
    "       ```\n",
    "       filename(without .txt at end) the speech present in the file, e.g., words separated by spaces\n",
    "       filename N ... words ....\n",
    "       ```\n",
    "   3. Then convert all \\*.flac files to \\*.wav files, using `flac2wav`\n",
    "   4. Remove all the flac files and remove the \\*.trans.txt files\n",
    "   5. Run this code in the notebook below to generate the associated \\*.txt file to go along with each \\*.wav file.\n",
    "   6. Move all the \\*.wav and \\*.txt files into a single folder, e.g., `LibriSpeech/train-clean-all` \n",
    "   7. Repeat for test and dev\n",
    "   \n",
    "Once complete, you have a dataset to run through [RNN-Tutorial](https://github.com/mrubash1/RNN-Tutorial.git)\n",
    "\n",
    "### References\n",
    "   1. [PyDub](https://github.com/jiaaro/pydub) - PyDub library\n",
    "   2. [A short reminder of how CTC works](https://towardsdatascience.com/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7)\n",
    "   3. [OpenSLR - LibriSpeech corpus](http://www.openslr.org/12)\n",
    "   4. [Hamsa's Deep Speech notebook](https://github.com/cosmoshsv/Deep-Speech/blob/master/DeepSpeech_RNN_Training.ipynb)\n",
    "   5. [LSTM's by example using TensorFlow](https://towardsdatascience.com/lstm-by-example-using-tensorflow-feb0c1968537) \n",
    "   6. [How to read an audio file using TensorFlow APIs](https://github.com/tensorflow/tensorflow/issues/28237)\n",
    "   7. [Audio spectrograms in TensorFlow](https://mauri870.github.io/blog/posts/audio-spectrograms-in-tensorflow/)\n",
    "   8. [Reading audio files using TensorFlow](https://github.com/tensorflow/tensorflow/issues/32382)\n",
    "   9. [TensorFlow's decode_wav API](https://www.tensorflow.org/api_docs/python/tf/audio/decode_wav)\n",
    "   10. [Speech Recognition](https://towardsdatascience.com/speech-recognition-analysis-f03ff9ce78e9)\n",
    "   11. [Using TensorFlow's audio ops](https://stackoverflow.com/questions/48660391/using-tensorflow-contrib-framework-python-ops-audio-ops-audio-spectrogram-to-gen)\n",
    "   12. [LSTM by Example - Towards Data Science](https://towardsdatascience.com/lstm-by-example-using-tensorflow-feb0c1968537)\n",
    "   13. [Training your Own Model -  DeepSpeech](https://deepspeech.readthedocs.io/en/v0.7.3/TRAINING.html)\n",
    "   14. [Understanding LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "   15. [Implementing  LSTMs](https://apaszke.github.io/lstm-explained.html)\n",
    "   16. [Mel Frequency Cepstral Coefficient](http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/)\n",
    "   17. [TensorFlow - Extract Every Other Element](https://stackoverflow.com/questions/46721407/tensorflow-extract-every-other-element)\n",
    "   18. [Plotting MFCCs in TensorFlow](https://stackoverflow.com/questions/47056432/is-it-possible-to-get-exactly-the-same-results-from-tensorflow-mfcc-and-librosa)\n",
    "   19. [MFCCs in TensorFlow](https://kite.com/python/docs/tensorflow.contrib.slim.rev_block_lib.contrib_framework_ops.audio_ops.mfcc)\n",
    "   20. [How to train Baidu's Deep Speech Model with Kur](https://blog.deepgram.com/how-to-train-baidus-deepspeech-model-with-kur/)\n",
    "   21. [Silicon Valley Data Science SVDS - RNN Tutorial](https://www.svds.com/tensorflow-rnn-tutorial/)\n",
    "   22. [Streaming RNNs with TensorFlow](https://hacks.mozilla.org/2018/09/speech-recognition-deepspeech/)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../libs/basic_units/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.audio import decode_wav\n",
    "from tensorflow.contrib.framework.python.ops import audio_ops\n",
    "from tensorflow.python.ops import ctc_ops\n",
    "from tensorflow.data.experimental import AUTOTUNE \n",
    "from pydub import AudioSegment\n",
    "import scipy.io.wavfile as wav\n",
    "from python_speech_features import mfcc\n",
    "from tqdm.notebook import tqdm\n",
    "import shutil\n",
    "import urllib.request\n",
    "import tarfile\n",
    "from basic_units import cm, inch\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import unicodedata\n",
    "import codecs\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SPACE_TOKEN = '<space>'\n",
    "SPACE_INDEX = 0\n",
    "FIRST_INDEX = ord('a') - 1  # 0 is reserved to space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_url = \"http://www.openslr.org/resources/12/train-clean-100.tar.gz\"\n",
    "dev_url = \"http://www.openslr.org/resources/12/dev-clean.tar.gz\"\n",
    "test_url = \"http://www.openslr.org/resources/12/test-clean.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_data_path = \"../data/LibriSpeech\"\n",
    "train_path = speech_data_path + \"/train-clean-100\"\n",
    "dev_path = speech_data_path + \"/dev-clean\"\n",
    "test_path = speech_data_path + \"/test-clean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BiRNN_model(batch_x, seq_length, n_input, n_context):\n",
    "    dropout = [0.05, 0.05, 0.05, 0.0, 0.0, 0.05]\n",
    "    relu_clip = 20\n",
    "\n",
    "    b1_stddev = 0.046875\n",
    "    h1_stddev = 0.046875\n",
    "    b2_stddev = 0.046875\n",
    "    h2_stddev = 0.046875\n",
    "    b3_stddev = 0.046875\n",
    "    h3_stddev = 0.046875\n",
    "    b5_stddev = 0.046875\n",
    "    h5_stddev = 0.046875\n",
    "    b6_stddev = 0.046875\n",
    "    h6_stddev = 0.046875\n",
    "\n",
    "    n_hidden_1 = 1024\n",
    "    n_hidden_2 = 1024\n",
    "    n_hidden_5 = 1024\n",
    "    n_cell_dim = 1024\n",
    "\n",
    "    n_hidden_3 = 2048\n",
    "    n_hidden_6 = 29    \n",
    "    n_character = 29\n",
    "\n",
    "    # Input shape: [batch_size, n_steps, n_input + 2*n_input*n_context]\n",
    "    batch_x_shape = tf.shape(batch_x)\n",
    "\n",
    "    # Reshaping `batch_x` to a tensor with shape `[n_steps*batch_size, n_input + 2*n_input*n_context]`.\n",
    "    # This is done to prepare the batch for input into the first layer which expects a tensor of rank `2`.\n",
    "\n",
    "    # Permute n_steps and batch_size\n",
    "    batch_x = tf.transpose(batch_x, [1, 0, 2])\n",
    "    # Reshape to prepare input for first layer\n",
    "    batch_x = tf.reshape(batch_x,\n",
    "                         [-1, n_input + 2 * n_input * n_context])  # (n_steps*batch_size, n_input + 2*n_input*n_context)\n",
    "\n",
    "    # The next three blocks will pass `batch_x` through three hidden layers with\n",
    "    # clipped RELU activation and dropout.\n",
    "\n",
    "    # 1st layer\n",
    "    with tf.name_scope('fc1'):\n",
    "        b1 = tf.get_variable(name='b1', shape=[n_hidden_1], initializer=tf.random_normal_initializer(stddev=b1_stddev))\n",
    "        h1 = tf.get_variable(name='h1', shape=[n_input + 2 * n_input * n_context, n_hidden_1],\n",
    "                             initializer=tf.random_normal_initializer(stddev=h1_stddev))\n",
    "        layer_1 = tf.minimum(tf.nn.relu(tf.add(tf.matmul(batch_x, h1), b1)), relu_clip)\n",
    "        layer_1 = tf.nn.dropout(layer_1, (1.0 - dropout[0]))\n",
    "\n",
    "        tf.summary.histogram(\"weights\", h1)\n",
    "        tf.summary.histogram(\"biases\", b1)\n",
    "        tf.summary.histogram(\"activations\", layer_1)\n",
    "\n",
    "    # 2nd layer\n",
    "    with tf.name_scope('fc2'):\n",
    "        b2 = tf.get_variable(name='b2', shape=[n_hidden_2], initializer=tf.random_normal_initializer(stddev=b2_stddev))\n",
    "        h2 = tf.get_variable(name='h2', shape=[n_hidden_1, n_hidden_2], initializer=tf.random_normal_initializer(stddev=h2_stddev))\n",
    "        layer_2 = tf.minimum(tf.nn.relu(tf.add(tf.matmul(layer_1, h2), b2)), relu_clip)\n",
    "        layer_2 = tf.nn.dropout(layer_2, (1.0 - dropout[1]))\n",
    "\n",
    "        tf.summary.histogram(\"weights\", h2)\n",
    "        tf.summary.histogram(\"biases\", b2)\n",
    "        tf.summary.histogram(\"activations\", layer_2)\n",
    "\n",
    "    # 3rd layer\n",
    "    with tf.name_scope('fc3'):\n",
    "        b3 = tf.get_variable(name='b3', shape=[n_hidden_3], initializer=tf.random_normal_initializer(stddev=b3_stddev))\n",
    "        h3 = tf.get_variable(name='h3', shape=[n_hidden_2, n_hidden_3], initializer=tf.random_normal_initializer(stddev=h3_stddev))\n",
    "        layer_3 = tf.minimum(tf.nn.relu(tf.add(tf.matmul(layer_2, h3), b3)), relu_clip)\n",
    "        layer_3 = tf.nn.dropout(layer_3, (1.0 - dropout[2]))\n",
    "\n",
    "        tf.summary.histogram(\"weights\", h3)\n",
    "        tf.summary.histogram(\"biases\", b3)\n",
    "        tf.summary.histogram(\"activations\", layer_3)\n",
    "\n",
    "    # Create the forward and backward LSTM units. Inputs have length `n_cell_dim`.\n",
    "    # LSTM forget gate bias initialized at `1.0` (default), meaning less forgetting\n",
    "    # at the beginning of training (remembers more previous info)\n",
    "    with tf.name_scope('lstm'):\n",
    "        # Forward direction cell:\n",
    "        lstm_fw_cell = tf.contrib.rnn.BasicLSTMCell(n_cell_dim, forget_bias=1.0, state_is_tuple=True)\n",
    "        lstm_fw_cell = tf.contrib.rnn.DropoutWrapper(lstm_fw_cell,\n",
    "                                                     input_keep_prob=1.0 - dropout[3],\n",
    "                                                     output_keep_prob=1.0 - dropout[3],\n",
    "                                                     # seed=random_seed,\n",
    "                                                     )\n",
    "        # Backward direction cell:\n",
    "        lstm_bw_cell = tf.contrib.rnn.BasicLSTMCell(n_cell_dim, forget_bias=1.0, state_is_tuple=True)\n",
    "        lstm_bw_cell = tf.contrib.rnn.DropoutWrapper(lstm_bw_cell,\n",
    "                                                     input_keep_prob=1.0 - dropout[4],\n",
    "                                                     output_keep_prob=1.0 - dropout[4],\n",
    "                                                     # seed=random_seed,\n",
    "                                                     )\n",
    "\n",
    "        # `layer_3` is now reshaped into `[n_steps, batch_size, 2*n_cell_dim]`,\n",
    "        # as the LSTM BRNN expects its input to be of shape `[max_time, batch_size, input_size]`.\n",
    "        layer_3 = tf.reshape(layer_3, [-1, batch_x_shape[0], n_hidden_3])\n",
    "\n",
    "        # Now we feed `layer_3` into the LSTM BRNN cell and obtain the LSTM BRNN output.\n",
    "        outputs, output_states = tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_fw_cell,\n",
    "                                                                 cell_bw=lstm_bw_cell,\n",
    "                                                                 inputs=layer_3,\n",
    "                                                                 dtype=tf.float32,\n",
    "                                                                 time_major=True,\n",
    "                                                                 sequence_length=seq_length)\n",
    "\n",
    "        tf.summary.histogram(\"activations\", outputs)\n",
    "\n",
    "        # Reshape outputs from two tensors each of shape [n_steps, batch_size, n_cell_dim]\n",
    "        # to a single tensor of shape [n_steps*batch_size, 2*n_cell_dim]\n",
    "        outputs = tf.concat(outputs, 2)\n",
    "        outputs = tf.reshape(outputs, [-1, 2 * n_cell_dim])\n",
    "\n",
    "    with tf.name_scope('fc5'):\n",
    "        # Now we feed `outputs` to the fifth hidden layer with clipped RELU activation and dropout\n",
    "        b5 = tf.get_variable(name='b5', shape=[n_hidden_5], initializer=tf.random_normal_initializer(stddev=b5_stddev))\n",
    "        h5 = tf.get_variable(name='h5', shape=[(2 * n_cell_dim), n_hidden_5], initializer=tf.random_normal_initializer(stddev=h5_stddev))\n",
    "        layer_5 = tf.minimum(tf.nn.relu(tf.add(tf.matmul(outputs, h5), b5)), relu_clip)\n",
    "        layer_5 = tf.nn.dropout(layer_5, (1.0 - dropout[5]))\n",
    "\n",
    "        tf.summary.histogram(\"weights\", h5)\n",
    "        tf.summary.histogram(\"biases\", b5)\n",
    "        tf.summary.histogram(\"activations\", layer_5)\n",
    "\n",
    "    with tf.name_scope('fc6'):\n",
    "        # Now we apply the weight matrix `h6` and bias `b6` to the output of `layer_5`\n",
    "        # creating `n_classes` dimensional vectors, the logits.\n",
    "        b6 = tf.get_variable(name='b6', shape=[n_hidden_6], initializer=tf.random_normal_initializer(stddev=b6_stddev))\n",
    "        h6 = tf.get_variable(name='h6', shape=[n_hidden_5, n_hidden_6], initializer=tf.random_normal_initializer(stddev=h6_stddev))\n",
    "        layer_6 = tf.add(tf.matmul(layer_5, h6), b6)\n",
    "\n",
    "        tf.summary.histogram(\"weights\", h6)\n",
    "        tf.summary.histogram(\"biases\", b6)\n",
    "        tf.summary.histogram(\"activations\", layer_6)\n",
    "\n",
    "    # Finally we reshape layer_6 from a tensor of shape [n_steps*batch_size, n_hidden_6]\n",
    "    # to the slightly more useful shape [n_steps, batch_size, n_hidden_6].\n",
    "    # Note, that this differs from the input in that it is time-major.\n",
    "    layer_6 = tf.reshape(layer_6, [-1, batch_x_shape[0], n_hidden_6])\n",
    "\n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "    # Output shape: [n_steps, batch_size, n_hidden_6]\n",
    "    return layer_6, summary_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio(audio_filename):\n",
    "    # Load wav files\n",
    "    fs, audio = wav.read(audio_filename)\n",
    "\n",
    "    # Get mfcc coefficients\n",
    "    orig_inputs = mfcc(audio, samplerate=fs, numcep=numcep)\n",
    "\n",
    "    # We only keep every second feature (BiRNN stride = 2)\n",
    "    orig_inputs = orig_inputs[::2]\n",
    "\n",
    "    # For each time slice of the training set, we need to copy the context this makes\n",
    "    # the numcep dimensions vector into a numcep + 2*numcep*numcontext dimensions\n",
    "    # because of:\n",
    "    #  - numcep dimensions for the current mfcc feature set\n",
    "    #  - numcontext*numcep dimensions for each of the past and future (x2) mfcc feature set\n",
    "    # => so numcep + 2*numcontext*numcep\n",
    "    train_inputs = np.array([], np.float32)\n",
    "    train_inputs.resize((orig_inputs.shape[0], numcep + 2 * numcep * numcontext))\n",
    "\n",
    "    # Prepare pre-fix post fix context\n",
    "    empty_mfcc = np.array([])\n",
    "    empty_mfcc.resize((numcep))\n",
    "\n",
    "    # Prepare train_inputs with past and future contexts\n",
    "    time_slices = range(train_inputs.shape[0])\n",
    "    context_past_min = time_slices[0] + numcontext\n",
    "    context_future_max = time_slices[-1] - numcontext\n",
    "    for time_slice in time_slices:\n",
    "        # Reminder: array[start:stop:step]\n",
    "        # slices from indice |start| up to |stop| (not included), every |step|\n",
    "\n",
    "        # Add empty context data of the correct size to the start and end\n",
    "        # of the MFCC feature matrix\n",
    "\n",
    "        # Pick up to numcontext time slices in the past, and complete with empty\n",
    "        # mfcc features\n",
    "        need_empty_past = max(0, (context_past_min - time_slice))\n",
    "        empty_source_past = list(empty_mfcc for empty_slots in range(need_empty_past))\n",
    "        data_source_past = orig_inputs[max(0, time_slice - numcontext):time_slice]\n",
    "        assert(len(empty_source_past) + len(data_source_past) == numcontext)\n",
    "\n",
    "        # Pick up to numcontext time slices in the future, and complete with empty\n",
    "        # mfcc features\n",
    "        need_empty_future = max(0, (time_slice - context_future_max))\n",
    "        empty_source_future = list(empty_mfcc for empty_slots in range(need_empty_future))\n",
    "        data_source_future = orig_inputs[time_slice + 1:time_slice + numcontext + 1]\n",
    "        assert(len(empty_source_future) + len(data_source_future) == numcontext)\n",
    "\n",
    "        if need_empty_past:\n",
    "            past = np.concatenate((empty_source_past, data_source_past))\n",
    "        else:\n",
    "            past = data_source_past\n",
    "\n",
    "        if need_empty_future:\n",
    "            future = np.concatenate((data_source_future, empty_source_future))\n",
    "        else:\n",
    "            future = data_source_future\n",
    "\n",
    "        past = np.reshape(past, numcontext * numcep)\n",
    "        now = orig_inputs[time_slice]\n",
    "        future = np.reshape(future, numcontext * numcep)\n",
    "\n",
    "        train_inputs[time_slice] = np.concatenate((past, now, future))\n",
    "        assert(len(train_inputs[time_slice]) == numcep + 2 * numcep * numcontext)\n",
    "\n",
    "    # Scale/standardize the inputs\n",
    "    # This can be done more efficiently in the TensorFlow graph\n",
    "    train_inputs = (train_inputs - np.mean(train_inputs)) / np.std(train_inputs)\n",
    "    return train_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_txt_file(txt_file, remove_apostrophe=True):\n",
    "    \"\"\"\n",
    "    Given a path to a text file, return contents with unsupported characters removed.\n",
    "    \"\"\"\n",
    "    with codecs.open(txt_file, encoding=\"utf-8\") as open_txt_file:\n",
    "        return normalize_text(open_txt_file.read(), remove_apostrophe=remove_apostrophe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(original, remove_apostrophe=True):\n",
    "    \"\"\"\n",
    "    Given a Python string ``original``, remove unsupported characters.\n",
    "    The only supported characters are letters and apostrophes.\n",
    "    \"\"\"\n",
    "    # convert any unicode characters to ASCII equivalent\n",
    "    # then ignore anything else and decode to a string\n",
    "    result = unicodedata.normalize(\"NFKD\", original).encode(\"ascii\", \"ignore\").decode()\n",
    "    if remove_apostrophe:\n",
    "        # remove apostrophes to keep contractions together\n",
    "        result = result.replace(\"'\", \"\")\n",
    "    # return lowercase alphabetic characters and apostrophes (if still present)\n",
    "    return re.sub(\"[^a-zA-Z']+\", ' ', result).strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_char_array(original):\n",
    "    \"\"\"\n",
    "    Given a Python string ``original``, map characters\n",
    "    to integers and return a numpy array representing the processed string.\n",
    "    This function has been modified from Mozilla DeepSpeech:\n",
    "    https://github.com/mozilla/DeepSpeech/blob/master/util/text.py\n",
    "    # This Source Code Form is subject to the terms of the Mozilla Public\n",
    "    # License, v. 2.0. If a copy of the MPL was not distributed with this\n",
    "    # file, You can obtain one at http://mozilla.org/MPL/2.0/.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create list of sentence's words w/spaces replaced by ''\n",
    "    result = original.replace(' ', '  ')\n",
    "    result = result.split(' ')\n",
    "\n",
    "    # Tokenize words into letters adding in SPACE_TOKEN where required\n",
    "    result = np.hstack([SPACE_TOKEN if xt == '' else list(xt) for xt in result])\n",
    "\n",
    "    # Return characters mapped into indicies\n",
    "    return np.asarray([SPACE_INDEX if xt == SPACE_TOKEN else ord(xt) - FIRST_INDEX for xt in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_tuple_from(sequences, dtype=np.int32):\n",
    "    \"\"\"\n",
    "    Create a sparse representention of ``sequences``.\n",
    "    Args:\n",
    "        sequences: a list of lists of type dtype where each element is a sequence\n",
    "    Returns:\n",
    "        A tuple with (indices, values, shape)\n",
    "    This function has been modified from Mozilla DeepSpeech:\n",
    "    https://github.com/mozilla/DeepSpeech/blob/master/util/text.py\n",
    "    # This Source Code Form is subject to the terms of the Mozilla Public\n",
    "    # License, v. 2.0. If a copy of the MPL was not distributed with this\n",
    "    # file, You can obtain one at http://mozilla.org/MPL/2.0/.\n",
    "    \"\"\"\n",
    "\n",
    "    indices = []\n",
    "    values = []\n",
    "\n",
    "    for n, seq in enumerate(sequences):\n",
    "        indices.extend(zip([n] * len(seq), range(len(seq))))\n",
    "        values.extend(seq)\n",
    "\n",
    "    indices = np.asarray(indices, dtype=np.int64)\n",
    "    values = np.asarray(values, dtype=dtype)\n",
    "    shape = np.asarray([len(sequences), indices.max(0)[1] + 1], dtype=np.int64)\n",
    "\n",
    "    # return tf.SparseTensor(indices=indices, values=values, shape=shape)\n",
    "    return indices, values, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, maxlen=None, dtype=np.float32,\n",
    "                  padding='post', truncating='post', value=0.):\n",
    "\n",
    "    '''\n",
    "    # From TensorLayer:\n",
    "    # http://tensorlayer.readthedocs.io/en/latest/_modules/tensorlayer/prepro.html\n",
    "    Pads each sequence to the same length of the longest sequence.\n",
    "        If maxlen is provided, any sequence longer than maxlen is truncated to\n",
    "        maxlen. Truncation happens off either the beginning or the end\n",
    "        (default) of the sequence. Supports post-padding (default) and\n",
    "        pre-padding.\n",
    "        Args:\n",
    "            sequences: list of lists where each element is a sequence\n",
    "            maxlen: int, maximum length\n",
    "            dtype: type to cast the resulting sequence.\n",
    "            padding: 'pre' or 'post', pad either before or after each sequence.\n",
    "            truncating: 'pre' or 'post', remove values from sequences larger\n",
    "            than maxlen either in the beginning or in the end of the sequence\n",
    "            value: float, value to pad the sequences to the desired value.\n",
    "        Returns:\n",
    "            numpy.ndarray: Padded sequences shape = (number_of_sequences, maxlen)\n",
    "            numpy.ndarray: original sequence lengths\n",
    "    '''\n",
    "    lengths = np.asarray([len(s) for s in sequences], dtype=np.int64)\n",
    "\n",
    "    nb_samples = len(sequences)\n",
    "    if maxlen is None:\n",
    "        maxlen = np.max(lengths)\n",
    "\n",
    "    # take the sample shape from the first non empty sequence\n",
    "    # checking for consistency in the main loop below.\n",
    "    sample_shape = tuple()\n",
    "    for s in sequences:\n",
    "        if len(s) > 0:\n",
    "            sample_shape = np.asarray(s).shape[1:]\n",
    "            break\n",
    "\n",
    "    x = (np.ones((nb_samples, maxlen) + sample_shape) * value).astype(dtype)\n",
    "    for idx, s in enumerate(sequences):\n",
    "        if len(s) == 0:\n",
    "            continue  # empty list was found\n",
    "        if truncating == 'pre':\n",
    "            trunc = s[-maxlen:]\n",
    "        elif truncating == 'post':\n",
    "            trunc = s[:maxlen]\n",
    "        else:\n",
    "            raise ValueError('Truncating type \"%s\" not understood' % truncating)\n",
    "\n",
    "        # check `trunc` has expected shape\n",
    "        trunc = np.asarray(trunc, dtype=dtype)\n",
    "        if trunc.shape[1:] != sample_shape:\n",
    "            raise ValueError('Shape of sample %s of sequence at position %s is different from expected shape %s' %\n",
    "                             (trunc.shape[1:], idx, sample_shape))\n",
    "\n",
    "        if padding == 'post':\n",
    "            x[idx, :len(trunc)] = trunc\n",
    "        elif padding == 'pre':\n",
    "            x[idx, -len(trunc):] = trunc\n",
    "        else:\n",
    "            raise ValueError('Padding type \"%s\" not understood' % padding)\n",
    "    return x, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transcripts = [file for file in glob.glob(train_path + \"/*/*/*.txt\")]\n",
    "dev_transcripts = [file for file in glob.glob(dev_path + \"/*/*/*.txt\")]\n",
    "test_transcripts = [file for file in glob.glob(test_path + \"/*/*/*.txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_audio_wav = [file for file in glob.glob(train_path + \"/*/*/*.wav\")]\n",
    "dev_audio_wav = [file for file in glob.glob(dev_path + \"/*/*/*.wav\")]\n",
    "test_audio_wav = [file for file in glob.glob(test_path + \"/*/*/*.wav\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../libs/RNN-Tutorial/src\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numcep=26\n",
    "numcontext=9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    filename =  '../data/LibriSpeech/train-clean-100/3486/166424/3486-166424-0004.wav'\n",
    "    raw_audio = tf.io.read_file(filename)\n",
    "    audio, fs = decode_wav(raw_audio)\n",
    "    print(np.shape(audio.eval()))\n",
    "    print(fs.eval())\n",
    "    \n",
    "    # Get mfcc coefficients\n",
    "    spectrogram = audio_ops.audio_spectrogram(\n",
    "            audio, window_size=1024,stride=64)\n",
    "    orig_inputs = audio_ops.mfcc(spectrogram, sample_rate=fs, dct_coefficient_count=numcep)\n",
    "    \n",
    "    audio_mfcc = orig_inputs.eval()\n",
    "    print(audio_mfcc)\n",
    "    print(np.shape(audio_mfcc))\n",
    "    hist_audio = np.histogram(audio_mfcc, bins=range(9 + 1))\n",
    "    plt.hist(hist_audio)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[]\n",
    "for i in np.arange(26):\n",
    "    labels.append(\"P\"+str(i+1))\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ind = np.arange(len(labels))\n",
    "width = 0.15\n",
    "colors = ['r', 'g', 'y', 'b', 'black']\n",
    "plots = []\n",
    "\n",
    "for i in range(0, 5):\n",
    "    Xs = np.asarray(np.abs(audio_mfcc[0][i])).reshape(-1)\n",
    "    p = ax.bar(ind + i*width, Xs, width, color=colors[i])\n",
    "    plots.append(p[0])\n",
    "\n",
    "xticks = ind + width / (audio_mfcc.shape[0])\n",
    "print(xticks)\n",
    "ax.legend(tuple(plots), ('S1', 'S2', 'S3', 'S4', 'S5'))\n",
    "ax.yaxis.set_units(inch)\n",
    "ax.autoscale_view()\n",
    "ax.set_xticks(xticks)\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "ax.set_ylabel('Normalized freq coumt')\n",
    "ax.set_xlabel('Pitch')\n",
    "ax.set_title('Normalized frequency counts for Various Sounds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    filename =  '../data/LibriSpeech/train-clean-100/3486/166424/3486-166424-0004.wav'\n",
    "    raw_audio = tf.io.read_file(filename)\n",
    "    audio, fs = decode_wav(raw_audio)\n",
    "    \n",
    "    wsize = 16384 #1024\n",
    "    stride = 448 #64\n",
    "    \n",
    "    \n",
    "    # Get mfcc coefficients\n",
    "    spectrogram = audio_ops.audio_spectrogram(\n",
    "            audio, window_size=wsize,stride=stride)\n",
    "    numcep=26\n",
    "    numcontext=9\n",
    "    orig_inputs = audio_ops.mfcc(spectrogram, sample_rate=fs, dct_coefficient_count=numcep) \n",
    "    orig_inputs = orig_inputs[:,::2]\n",
    "    \n",
    "    audio_mfcc = orig_inputs.eval()\n",
    "    print(audio_mfcc)\n",
    "    print(np.shape(audio_mfcc))\n",
    "\n",
    "    train_inputs = np.array([], np.float32)\n",
    "    train_inputs.resize((audio_mfcc.shape[1], numcep + 2 * numcep * numcontext))\n",
    "\n",
    "    # Prepare pre-fix post fix context\n",
    "    empty_mfcc = np.array([])\n",
    "    empty_mfcc.resize((numcep))\n",
    "    empty_mfcc = tf.convert_to_tensor(empty_mfcc, dtype=tf.float32)\n",
    "    empty_mfcc_ev = empty_mfcc.eval()\n",
    "    \n",
    "    # Prepare train_inputs with past and future contexts\n",
    "    # This code always takes 9 time steps previous and 9 time steps in the future along with the current time step\n",
    "    time_slices = range(train_inputs.shape[0])\n",
    "    context_past_min = time_slices[0] + numcontext #starting min point for past content, has to be at least 9 ts\n",
    "    context_future_max = time_slices[-1] - numcontext  #ending point  max for future content, size time slices - 9ts\n",
    "\n",
    "    for time_slice in tqdm(time_slices):\n",
    "        #print('time slice %d ' % (time_slice))\n",
    "        # Reminder: array[start:stop:step]\n",
    "        # slices from indice |start| up to |stop| (not included), every |step|\n",
    "\n",
    "        # Add empty context data of the correct size to the start and end\n",
    "        # of the MFCC feature matrix\n",
    "\n",
    "        # Pick up to numcontext time slices in the past, and complete with empty\n",
    "        # mfcc features\n",
    "        need_empty_past = max(0, (context_past_min - time_slice))\n",
    "        empty_source_past = np.asarray([empty_mfcc_ev for empty_slots in range(need_empty_past)])\n",
    "        data_source_past = orig_inputs[0][max(0, time_slice - numcontext):time_slice]\n",
    "        assert(len(empty_source_past) + data_source_past.eval().shape[0] == numcontext)\n",
    "\n",
    "        # Pick up to numcontext time slices in the future, and complete with empty\n",
    "        # mfcc features\n",
    "        need_empty_future = max(0, (time_slice - context_future_max))\n",
    "        empty_source_future = np.asarray([empty_mfcc_ev for empty_slots in range(need_empty_future)])\n",
    "        data_source_future = orig_inputs[0][time_slice + 1:time_slice + numcontext + 1]\n",
    "        assert(len(empty_source_future) + data_source_future.eval().shape[0] == numcontext)\n",
    "        \n",
    "        # pad if needed for the past or future, or else simply take past and future\n",
    "        if need_empty_past:\n",
    "            past = tf.concat([tf.cast(empty_source_past, tf.float32), tf.cast(data_source_past, tf.float32)], 0)\n",
    "        else:\n",
    "            past = data_source_past\n",
    "\n",
    "        if need_empty_future:\n",
    "            future = tf.concat([tf.cast(data_source_future, tf.float32), tf.cast(empty_source_future, tf.float32)], 0)\n",
    "        else:\n",
    "            future = data_source_future\n",
    "\n",
    "\n",
    "        past = tf.reshape(past, [numcontext*numcep])\n",
    "        now = orig_inputs[0][time_slice]\n",
    "        future  = tf.reshape(future, [numcontext*numcep])\n",
    "\n",
    "        train_inputs[time_slice] = np.concatenate((past.eval(), now.eval(), future.eval()))\n",
    "        assert(train_inputs[time_slice].shape[0] == numcep + 2*numcep*numcontext)\n",
    "        \n",
    "    train_inputs = (train_inputs - np.mean(train_inputs)) / np.std(train_inputs)\n",
    "    print('Train inputs shape %s ' % str(np.shape(train_inputs)))\n",
    "    print('Train inputs '+str(train_inputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename =  '../data/LibriSpeech/train-clean-100/3486/166424/3486-166424-0004.txt'\n",
    "txt_file = normalize_txt_file(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript =  text_to_char_array(txt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_sparse = sparse_tuple_from(np.asarray([transcript]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, t_length = pad_sequences([train_inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g: log filter bank or MFCC features\n",
    "# shape = [batch_size, max_stepsize, n_input + (2 * n_input * n_context)]\n",
    "# the batch_size and max_stepsize can vary along each step\n",
    "input_tensor  = tf.placeholder(tf.float32, [None, None, numcep + (2 * numcep * numcontext)], name='input')\n",
    "\n",
    "# 1d array of size [batch_size]\n",
    "seq_length = tf.placeholder(tf.int32, [None], name='seq_length')\n",
    "\n",
    "# Use sparse_placeholder; will generate a SparseTensor, required by ctc_loss op.\n",
    "targets = tf.sparse_placeholder(tf.int32, name='targets')\n",
    "    \n",
    "logits, summary_op = BiRNN_model(input_tensor, tf.to_int64(seq_length), numcep, numcontext)\n",
    "\n",
    "        \n",
    "total_loss = ctc_ops.ctc_loss(targets, logits, seq_length)\n",
    "avg_loss = tf.reduce_mean(total_loss)\n",
    "loss_summary = tf.summary.scalar(\"avg_loss\", avg_loss)\n",
    "cost_placeholder = tf.placeholder(dtype=tf.float32, shape=[])\n",
    "train_cost_op = tf.summary.scalar(\n",
    "                \"train_avg_loss\", cost_placeholder)\n",
    "\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "epsilon = 1e-8\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                   beta1=beta1,\n",
    "                                   beta2=beta2,\n",
    "                                   epsilon=epsilon)\n",
    "\n",
    "train_op = optimizer.minimize(avg_loss)\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=10\n",
    "train_size=25\n",
    "train_audio_ds = tf.data.Dataset.from_tensor_slices(train_audio_wav[0:train_size-1])\n",
    "train_audio_ds = train_audio_ds.batch(BATCH_SIZE)\n",
    "train_audio_ds = train_audio_ds.shuffle(buffer_size=train_size)\n",
    "train_audio_ds = train_audio_ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_cost = 0.\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(0, num_epochs):\n",
    "        iter = train_audio_ds.make_one_shot_iterator()\n",
    "        batch_num = 0\n",
    "        iter_op = iter.get_next()\n",
    "        trans_batch = []\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                train_batch = sess.run(iter_op)\n",
    "                trans_batch = ['..' + t.decode('utf-8').split('.')[2] + \".txt\" for t in train_batch]\n",
    "                train_batch = [process_audio(t) for t in train_batch]\n",
    "                train, t_length = pad_sequences(train_batch)\n",
    "        \n",
    "                trans_batch = [normalize_txt_file(t) for t in trans_batch]\n",
    "                trans_batch = [text_to_char_array(t) for t in trans_batch]\n",
    "                transcript_sparse = sparse_tuple_from(np.asarray(trans_batch))\n",
    "\n",
    "                feed = {input_tensor: train,\n",
    "                        targets: transcript_sparse,\n",
    "                        seq_length: t_length}\n",
    "                batch_cost, _ = sess.run([avg_loss, train_op], feed_dict=feed)\n",
    "                train_cost += batch_cost * BATCH_SIZE\n",
    "                batch_num += 1\n",
    "                print('Batch cost: %.2f' % (batch_cost))\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                train_cost /= train_size\n",
    "                print('Epoch %d | Train cost: %.2f' % (epoch, train_cost))\n",
    "                break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
